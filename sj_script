git clone https://github.com/pubmedqa/pubmedqa.git

wget https://huggingface.co/bofenghuang/Meta-Llama-3-8B/resolve/1460c22666392e470910ce3d44ffeb2ab7dbd4df/original/tokenizer.model


python split_dataset.py pqal

; python3 llama/preprocess_pubmeqa.py --input_dir pubmedqa/data/train_data --output_file pubmedqa/pubmeqa_preprocessed.txt
python3 llama/preprocess_pubmeqa.py --input_dir pubmedqa/data --output_file  pubmedqa/pubmeqa_preprocessed.txt



python3 llama/training.py \
        --ckpt_dir Meta-Llama-3-8B-Instruct \
        --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \
        --data_path pubmedqa/pubmeqa_preprocessed_train.txt \
        --output_dir checkpoint_dir/finetuned_pubmeqa \
        --batch_size 8

accelerate launch --mixed_precision=fp16 llama/training.py \
        --ckpt_dir Meta-Llama-3-8B-Instruct \
        --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \
        --data_path pubmedqa/pubmeqa_preprocessed_train.txt \
        --output_dir checkpoint_dir/finetuned_pubmeqa \
        --batch_size 1 \
        --gradient_accumulation_steps 1

DS_BUILD_CPU_ADAM=0 accelerate launch --use_deepspeed --deepspeed_config_file deepspeed_config.json llama/training.py \
        --ckpt_dir Meta-Llama-3-8B-Instruct \
        --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \
        --data_path pubmedqa/pubmeqa_preprocessed_train.txt \
        --output_dir checkpoint_dir/finetuned_pubmeqa \
        --batch_size 1 \
        --gradient_accumulation_steps 1


accelerate launch --use_deepspeed --deepspeed_config_file deepspeed_config.json llama/training.py \
        --ckpt_dir Meta-Llama-3-8B-Instruct \
        --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \
        --data_path pubmedqa/pubmeqa_preprocessed_train.txt \
        --output_dir checkpoint_dir/finetuned_pubmeqa \
        --batch_size 1 \
        --gradient_accumulation_steps 1

그리고 추론이 완성되었으면, 체크포인팅 된걸 복구해서 추론성능 테스트하는 코드 llama/inference.py 만들어줘. 
추론 데이터 셋은 json 파일로 pubmedqa/test_set.json꼴로 되어있어

pip install numpy==2.0
pip install torch==2.1.2


accelerate launch /home/elicer/llama3/llama/training.py \
        --ckpt_dir Meta-Llama-3-8B-Instruct \
        --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \
        --data_path pubmedqa/pubmeqa_preprocessed_train.txt \
        --output_dir checkpoints/finetuned_pubmeqa \
        --batch_size 1 \
        --epochs 3 \
        --learning_rate 1e-5


        
accelerate launch llama/training.py \
            --ckpt_dir Meta-Llama-3-8B-Instruct \
            --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \
            --data_path pubmedqa/pubmeqa_preprocessed_train.txt \
            --output_dir checkpoint_dir/finetuned_pubmeqa \
            --batch_size 1 \
            --gradient_accumulation_steps 1

wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb

sudo dpkg -i cuda-keyring_1.0-1_all.deb
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-1

export PATH=/usr/local/cuda-12.1/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}